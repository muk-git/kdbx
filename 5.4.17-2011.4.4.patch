diff --git a/Makefile b/Makefile
index 584b931b8950..d766fefdffc8 100644
--- a/Makefile
+++ b/Makefile
@@ -460,7 +460,9 @@ KBUILD_CFLAGS   := -Wall -Wundef -Werror=strict-prototypes -Wno-trigraphs \
 		   -Werror=implicit-function-declaration -Werror=implicit-int \
 		   -Wno-format-security \
 		   -fmax-errors=4 -Werror=return-type -Werror=int-conversion \
-		   -Werror=misleading-indentation       \
+		   -Werror=misleading-indentation -Wno-unused-variable       \
+		   -fno-ipa-sra	-Wno-stringop-truncation -Wno-unused-function \
+		   -Wno-packed-not-aligned \
 		   -std=gnu89
 KBUILD_CPPFLAGS := -D__KERNEL__
 KBUILD_AFLAGS_KERNEL :=
@@ -624,6 +626,7 @@ drivers-$(CONFIG_KERNEL_HEADER_TEST) += include/
 net-y		:= net/
 libs-y		:= lib/
 core-y		:= usr/
+kdbx-y         	:= kdbx/
 virt-y		:= virt/
 endif # KBUILD_EXTMOD
 
@@ -838,6 +841,12 @@ ifdef CONFIG_DYNAMIC_FTRACE
 endif
 endif
 
+# ifeq ("$(origin kdbx)", "command line")
+ifeq ("$(kdbx)", "y")
+KBUILD_CFLAGS  += -DCONFIG_KDBX
+KBUILD_AFLAGS  += -DCONFIG_KDBX
+endif
+
 # We trigger additional mismatches with less inlining
 ifdef CONFIG_DEBUG_SECTION_MISMATCH
 KBUILD_CFLAGS += $(call cc-option, -fno-inline-functions-called-once)
@@ -1031,6 +1040,10 @@ export MODORDER := $(extmod-prefix)modules.order
 ifeq ($(KBUILD_EXTMOD),)
 core-y		+= kernel/ certs/ mm/ fs/ ipc/ security/ crypto/ block/
 
+ifeq ("$(kdbx)", "y")
+core-y         += kdbx/
+endif
+
 vmlinux-dirs	:= $(patsubst %/,%,$(filter %/, $(init-y) $(init-m) \
 		     $(core-y) $(core-m) $(drivers-y) $(drivers-m) \
 		     $(net-y) $(net-m) $(libs-y) $(libs-m) $(virt-y)))
diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S
index 6f5a2d7b600e..d2257f7004e1 100644
--- a/arch/x86/entry/entry_64.S
+++ b/arch/x86/entry/entry_64.S
@@ -1403,6 +1403,94 @@ END(error_exit)
  *	      when PAGE_TABLE_ISOLATION is in use.  Do not clobber.
  */
 ENTRY(nmi)
+#ifdef CONFIG_KDBX
+        /* kdbx doesn't support nested NMI. The nested NMI code below copies
+         * eflags from orig place to new place, thus setting eflags to TF
+         * in kdbx does not work. Besides the code keeps constantly changing 
+         */
+
+        /* the code here is remapped, so %rip relative will NOT work */
+        // cmpl $0, kdbx_ignore_nmi(%rip)
+	// jnz kdbx_ignore_nmi
+
+#ifdef CONFIG_KDBX_FOR_XEN_DOM0
+        /* pop extra rcx and r11 from the stack due to syscall/sysretq calls */
+        PARAVIRT_ADJUST_EXCEPTION_FRAME
+#endif
+        /* no error code is saved on stack by cpu in case of external ints or
+         * nmi's executing on ist stack. so sp is pointing to IP */
+        testb	$3, CS-RIP(%rsp)
+	jz .Lkdbx_nmi_from_kernel
+        swapgs
+        cld
+        pushq %rax
+	SWITCH_TO_KERNEL_CR3 scratch_reg=%rax
+        popq %rax
+
+.Lkdbx_nmi_from_kernel:
+        /* Documentation/x86/kernel-stacks: nmi uses idte.ist stack which is
+         * of size EXCEPTION_STKSZ. idte.ist points to exception_stacks[].
+         * cpu will switch to it unconditionally, so if new nmi, context of 
+         * old will be lost. Altho, x86 will not inject second nmi until the
+         * iret from first. However, another cpu is free to receive it.
+         * See arch/x86/include/asm/page_64_types.h for NMI stack size 
+         */
+        pushq $-1        /* fake orig_ax in pt_regs{} */
+
+        /* save rest of struct pt_regs. We could call paranoid_entry or 
+         * PUSH_AND_CLEAR_REGS, but they keep changing every other version */
+        pushq   %rdi            /* pt_regs->di */
+        pushq   %rsi            /* pt_regs->si */
+        pushq   %rdx            /* pt_regs->dx */
+        pushq   %rcx            /* pt_regs->cx */
+        pushq   %rax            /* pt_regs->ax */
+        pushq   %r8             /* pt_regs->r8 */
+        pushq   %r9             /* pt_regs->r9 */
+        pushq   %r10            /* pt_regs->r10 */
+        pushq   %r11            /* pt_regs->r11 */
+        pushq   %rbx            /* pt_regs->rbx */
+        pushq   %rbp            /* pt_regs->rbp */
+        pushq   %r12            /* pt_regs->r12 */
+        pushq   %r13            /* pt_regs->r13 */
+        pushq   %r14            /* pt_regs->r14 */
+        pushq   %r15            /* pt_regs->r15 */
+
+	movq	%rsp, %rdi
+	call kdbx_do_nmi
+
+        popq %r15
+        popq %r14
+        popq %r13
+        popq %r12
+        popq %rbp
+        popq %rbx
+        popq %r11
+        popq %r10
+        popq %r9
+        popq %r8
+        popq %rax
+        popq %rcx
+        popq %rdx
+        popq %rsi
+        popq %rdi
+	addq $8, %rsp	/* get rid of our fake orig_ax */
+
+        testb	$3, CS-RIP(%rsp)
+	jz  .Lkdbx_iretq
+        swapgs
+        pushq %rax
+        pushq %rdi
+        SWITCH_TO_USER_CR3_NOSTACK scratch_reg=%rax scratch_reg2=%rdi
+        popq %rdi
+        popq %rax
+
+.Lkdbx_iretq:
+        iretq
+
+// kdbx_ignore_nmi:
+END(nmi)
+#endif  /* CONFIG_KDBX */
+
 	UNWIND_HINT_IRET_REGS
 
 	/*
diff --git a/arch/x86/include/asm/page_64_types.h b/arch/x86/include/asm/page_64_types.h
index 288b065955b7..db378797d088 100644
--- a/arch/x86/include/asm/page_64_types.h
+++ b/arch/x86/include/asm/page_64_types.h
@@ -15,7 +15,11 @@
 #define THREAD_SIZE_ORDER	(2 + KASAN_STACK_ORDER)
 #define THREAD_SIZE  (PAGE_SIZE << THREAD_SIZE_ORDER)
 
+#ifdef CONFIG_KDBX
+#define EXCEPTION_STACK_ORDER (2)  /* seeing OF, heck, just make it 16k */
+#else
 #define EXCEPTION_STACK_ORDER (0 + KASAN_STACK_ORDER)
+#endif
 #define EXCEPTION_STKSZ (PAGE_SIZE << EXCEPTION_STACK_ORDER)
 
 #define IRQ_STACK_ORDER (2 + KASAN_STACK_ORDER)
diff --git a/arch/x86/kernel/doublefault.c b/arch/x86/kernel/doublefault.c
index d5c9b13bafdf..8694d163b52d 100644
--- a/arch/x86/kernel/doublefault.c
+++ b/arch/x86/kernel/doublefault.c
@@ -81,6 +81,9 @@ void df_debug(struct pt_regs *regs, long error_code)
 {
 	pr_emerg("PANIC: double fault, error_code: 0x%lx\n", error_code);
 	show_regs(regs);
+#ifdef CONFIG_KDBX
+        kdbxmain_fatal(regs, 8);  /* double fault : 8 == X86_TRAP_DF */
+#endif
 	panic("Machine halted.");
 }
 #endif
diff --git a/arch/x86/kernel/dumpstack.c b/arch/x86/kernel/dumpstack.c
index e07424e19274..ca022c3acd01 100644
--- a/arch/x86/kernel/dumpstack.c
+++ b/arch/x86/kernel/dumpstack.c
@@ -388,6 +388,10 @@ int __die(const char *str, struct pt_regs *regs, long err)
 	show_regs(regs);
 	print_modules();
 
+#ifdef CONFIG_KDBX
+        kdbxp("Kernel Panic... Entering kdbx\n");
+        kdbxmain_fatal(regs, err);
+#endif
 	if (notify_die(DIE_OOPS, str, regs, err,
 			current->thread.trap_nr, SIGSEGV) == NOTIFY_STOP)
 		return 1;
diff --git a/arch/x86/kernel/dumpstack_64.c b/arch/x86/kernel/dumpstack_64.c
index 87b97897a881..6f7e31817992 100644
--- a/arch/x86/kernel/dumpstack_64.c
+++ b/arch/x86/kernel/dumpstack_64.c
@@ -75,7 +75,9 @@ struct estack_pages {
  * info. The guard pages including the not mapped DB2 stack are zeroed
  * out.
  */
+#ifndef CONFIG_KDBX
 static const
+#endif
 struct estack_pages estack_pages[CEA_ESTACK_PAGES] ____cacheline_aligned = {
 	EPAGERANGE(DF),
 	EPAGERANGE(NMI),
diff --git a/arch/x86/kernel/setup.c b/arch/x86/kernel/setup.c
index 8f48bb8f2ceb..f76afb95ebba 100644
--- a/arch/x86/kernel/setup.c
+++ b/arch/x86/kernel/setup.c
@@ -904,6 +904,11 @@ void __init setup_arch(char **cmdline_p)
 	jump_label_init();
 	early_ioremap_init();
 
+#ifdef CONFIG_KDBX
+        kdbx_init(boot_command_line);
+        if ( strstr(boot_command_line, "earlykdbx") )
+            kdbx_trap_immed(KDBX_TRAP_NONFATAL);
+#endif
 	setup_olpc_ofw_pgd();
 
 	ROOT_DEV = old_decode_dev(boot_params.hdr.root_dev);
diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index b8d4e9c3c070..4d9f8f5155c0 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -33,6 +33,10 @@
 #include <asm/kexec.h>
 #include <asm/virtext.h>
 
+#ifdef CONFIG_KDBX
+#include <asm/irq_regs.h>
+#endif
+
 /*
  *	Some notes on x86 processor bugs affecting SMP operation:
  *
@@ -246,22 +250,36 @@ __visible void __irq_entry smp_reschedule_interrupt(struct pt_regs *regs)
 
 __visible void __irq_entry smp_call_function_interrupt(struct pt_regs *regs)
 {
+#ifdef CONFIG_KDBX
+        struct pt_regs *old_regs = set_irq_regs(regs);
+#endif
 	ipi_entering_ack_irq();
 	trace_call_function_entry(CALL_FUNCTION_VECTOR);
 	inc_irq_stat(irq_call_count);
 	generic_smp_call_function_interrupt();
 	trace_call_function_exit(CALL_FUNCTION_VECTOR);
 	exiting_irq();
+
+#ifdef CONFIG_KDBX
+        set_irq_regs(old_regs);
+#endif
 }
 
 __visible void __irq_entry smp_call_function_single_interrupt(struct pt_regs *r)
 {
+#ifdef CONFIG_KDBX
+        struct pt_regs *old_regs = set_irq_regs(r);
+#endif
 	ipi_entering_ack_irq();
 	trace_call_function_single_entry(CALL_FUNCTION_SINGLE_VECTOR);
 	inc_irq_stat(irq_call_count);
 	generic_smp_call_function_single_interrupt();
 	trace_call_function_single_exit(CALL_FUNCTION_SINGLE_VECTOR);
 	exiting_irq();
+
+#ifdef CONFIG_KDBX
+        set_irq_regs(old_regs);
+#endif
 }
 
 static int __init nonmi_ipi_setup(char *str)
diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 4bb0f8447112..a335056a93f1 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -318,9 +318,20 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code, unsign
 	static const char str[] = "double fault";
 	struct task_struct *tsk = current;
 
+#ifndef CONFIG_X86_ESPFIX64
+#ifdef CONFIG_KDBX
+        /* avoid mumbo jubmo, and just trap in kdbx */
+        kdbxmain_fatal(regs, X86_TRAP_DF);
+#endif
+#endif
+
 #ifdef CONFIG_X86_ESPFIX64
 	extern unsigned char native_irq_return_iret[];
 
+#ifdef CONFIG_KDBX
+        /* avoid mumbo jubmo, and just trap in kdbx */
+        kdbxmain_fatal(regs, X86_TRAP_DF);
+#endif
 	/*
 	 * If IRET takes a non-IST fault on the espfix64 stack, then we
 	 * end up promoting it to a doublefault.  In that case, take
@@ -520,6 +531,12 @@ do_general_protection(struct pt_regs *regs, long error_code)
 	const char *desc = "general protection fault";
 	struct task_struct *tsk;
 
+#ifdef CONFIG_KDBX
+        if (kdbx_session_begun) {
+            if ( kdbx_excp_fixup(regs, X86_TRAP_GP) == 0 )
+                return;
+}
+#endif
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
 	cond_local_irq_enable(regs);
 
@@ -580,6 +597,11 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 	if (poke_int3_handler(regs))
 		return;
 
+#ifdef CONFIG_KDBX
+        if (kdbx_handle_trap_entry(X86_TRAP_BP, regs))
+                return;
+#endif
+
 	/*
 	 * Use ist_enter despite the fact that we don't use an IST stack.
 	 * We can be called from a kprobe in non-CONTEXT_KERNEL kernel
@@ -713,6 +735,11 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	unsigned long dr6;
 	int si_code;
 
+#ifdef CONFIG_KDBX
+        if (kdbx_handle_trap_entry(X86_TRAP_DB, regs))
+                return;
+#endif
+
 	ist_enter(regs);
 
 	get_debugreg(dr6, 6);
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index 9f817b623a99..56644245a166 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -2481,6 +2481,10 @@ static __init int setup_vmcs_config(struct vmcs_config *vmcs_conf,
 
 	if (cpu_has_broken_vmx_preemption_timer())
 		_pin_based_exec_control &= ~PIN_BASED_VMX_PREEMPTION_TIMER;
+#ifdef CONFIG_KDBX
+        /* disable vmx preemption timer */
+	_pin_based_exec_control &= ~PIN_BASED_VMX_PREEMPTION_TIMER;
+#endif
 	if (!(_cpu_based_2nd_exec_control &
 		SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))
 		_pin_based_exec_control &= ~PIN_BASED_POSTED_INTR;
@@ -2998,7 +3002,7 @@ void vmx_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)
 	vmx->emulation_required = emulation_required(vcpu);
 }
 
-static int get_ept_level(struct kvm_vcpu *vcpu)
+int get_ept_level(struct kvm_vcpu *vcpu)
 {
 	if (cpu_has_vmx_ept_5levels() && (cpuid_maxphyaddr(vcpu) > 48))
 		return 5;
@@ -4715,6 +4719,10 @@ static int handle_exception_nmi(struct kvm_vcpu *vcpu)
 		kvm_queue_exception_e(vcpu, AC_VECTOR, error_code);
 		return 1;
 	case DB_VECTOR:
+#ifdef CONFIG_KDBX
+                if ( kdbx_handle_guest_trap(DB_VECTOR, vcpu) )
+                    return 1;   /* handled */
+#endif
 		dr6 = vmcs_readl(EXIT_QUALIFICATION);
 		if (!(vcpu->guest_debug &
 		      (KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))) {
@@ -4730,6 +4738,10 @@ static int handle_exception_nmi(struct kvm_vcpu *vcpu)
 		kvm_run->debug.arch.dr7 = vmcs_readl(GUEST_DR7);
 		/* fall through */
 	case BP_VECTOR:
+#ifdef CONFIG_KDBX
+                if ( kdbx_handle_guest_trap(BP_VECTOR, vcpu) )
+                    return 1;   /* handled */
+#endif
 		/*
 		 * Update instruction length as we may reinject #BP from
 		 * user space while in guest debugging mode. Reading it for
@@ -6263,6 +6275,11 @@ static void handle_exception_nmi_irqoff(struct vcpu_vmx *vmx)
 
 	/* We need to handle NMIs before interrupts are enabled */
 	if (is_nmi(vmx->exit_intr_info)) {
+#ifdef CONFIG_KDBX
+                /* nmi from kdb main cpu */
+                kdbx_handle_guest_trap(2, &vmx->vcpu);
+                return;
+#endif
 		kvm_before_interrupt(&vmx->vcpu);
 		asm("int $2");
 		kvm_after_interrupt(&vmx->vcpu);
@@ -6609,6 +6626,21 @@ static void vmx_vcpu_run(struct kvm_vcpu *vcpu)
 	 */
 	x86_spec_ctrl_set_guest(vmx->spec_ctrl, 0);
 
+#ifdef CONFIG_KDBX
+        if ( vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP ) {
+            int eb = vmcs_read32(EXCEPTION_BITMAP) | 1u << DB_VECTOR;
+            vmcs_write32(EXCEPTION_BITMAP, eb);
+            vmcs_writel(GUEST_RFLAGS, vmcs_readl(GUEST_RFLAGS) | X86_EFLAGS_TF);
+        }
+        if ( vcpu->guest_debug & KVM_GUESTDBG_USE_SW_BP ) {
+            int eb = vmcs_read32(EXCEPTION_BITMAP) | 1u << BP_VECTOR;
+            vmcs_write32(EXCEPTION_BITMAP, eb);
+        }  else {
+            int eb = vmcs_read32(EXCEPTION_BITMAP) & ~(1u << BP_VECTOR);
+            vmcs_write32(EXCEPTION_BITMAP, eb);
+        }
+#endif
+
 	/* L1D Flush includes CPU buffer clear to mitigate MDS */
 	if (static_branch_unlikely(&vmx_l1d_should_flush))
 		vmx_l1d_flush(vcpu);
@@ -6706,6 +6738,19 @@ static void vmx_vcpu_run(struct kvm_vcpu *vcpu)
 	vmx->loaded_vmcs->launched = 1;
 	vmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);
 
+#ifdef CONFIG_KDBX
+        /* DO not clear the TF unconditionally, guest could be setting it.
+         * do it only if host is doing guest debugging */
+        if ( vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP ) {
+            int eb = vmcs_read32(EXCEPTION_BITMAP) & ~(1u << DB_VECTOR);
+            vmcs_write32(EXCEPTION_BITMAP, eb);
+            vmcs_writel(GUEST_RFLAGS,vmcs_readl(GUEST_RFLAGS) & ~X86_EFLAGS_TF);
+        }
+        vmx_cache_reg(vcpu, VCPU_REGS_RIP);
+        vmx_cache_reg(vcpu, VCPU_REGS_RSP);
+        // vcpu->arch.regs[VCPU_REGS_RIP] = vmcs_readl(GUEST_RIP);
+        // vcpu->arch.regs[VCPU_REGS_RSP] = vmcs_readl(GUEST_RSP);
+#endif
 	vmx_recover_nmi_blocking(vmx);
 	vmx_complete_interrupts(vmx);
 }
@@ -7736,6 +7781,10 @@ static __init int hardware_setup(void)
 	if (enable_ept && !cpu_has_vmx_ept_2m_page())
 		kvm_disable_largepages();
 
+#ifdef CONFIG_KDBX
+        kvm_disable_largepages();
+#endif
+
 #if IS_ENABLED(CONFIG_HYPERV)
 	if (ms_hyperv.nested_features & HV_X64_NESTED_GUEST_MAPPING_FLUSH
 	    && enable_ept) {
@@ -8112,3 +8161,338 @@ static int __init vmx_init(void)
 	return 0;
 }
 module_init(vmx_init);
+
+
+#ifdef CONFIG_KDBX
+
+struct kvm_vcpu *vcpu_from_vmcs(struct vmcs *vmcs)
+{
+    int i;
+    struct list_head *lp;
+
+    if (vmcs == NULL)
+        return NULL;
+
+    /* vm_list part of struct kvm in kvm_host.h */
+    list_for_each(lp, &vm_list) {
+        struct kvm *kp = list_entry(lp, struct kvm, vm_list);  /* container of*/
+
+        for (i = 0; i < KVM_MAX_VCPUS; i++) {
+            struct vcpu_vmx *vx;
+            struct kvm_vcpu *vp = kp->vcpus[i];
+
+            if (vp == NULL)
+                continue;
+
+            vx = to_vmx(vp); 
+            if ( vx->loaded_vmcs && vx->loaded_vmcs->vmcs == vmcs )
+                return vp;
+        }
+    }
+    return NULL;
+}
+
+void kdbx_ret_curr_vcpu_info(struct kvm_vcpu **vpp, struct vmcs **vmcspp,
+                             struct vmcs **vmxapp)
+{
+    int ccpu = raw_smp_processor_id();
+    struct vmcs *vmcs = per_cpu(current_vmcs, ccpu);
+    struct vmcs *vmxa = per_cpu(vmxarea, ccpu);
+
+    if ( !enable_ept )
+        kdbxp(">>>> WARNING: ept is not enabled... \n");
+
+    if ( vmcspp )
+        *vmcspp = vmcs;
+    if ( vmxapp )
+        *vmxapp = vmxa;
+    if ( vpp )
+        *vpp = vcpu_from_vmcs(vmcs);
+}
+
+void kdbx_display_vvmx(struct kvm_vcpu *vp)
+{
+    struct vcpu_vmx *vv = (struct vcpu_vmx *)vp;
+
+    if ( vv == NULL )
+        return;
+
+    kdbxp("vmx:\n");
+    kdbxp("  fail:%x exit_reason:%x exit_intr_info:%x idt_vectoring_info:%x\n",
+          vv->fail, vv->exit_reason, vv->exit_intr_info,vv->idt_vectoring_info);
+    kdbxp("  vpid:%x emulation_required:%d posted_int_desc: %p\n",
+          vv->vpid, vv->emulation_required, &vv->pi_desc);
+    kdbxp("  host_kernel_gs_base:%016lx guest_kernel_gs_base:%016lx\n",
+          vv->msr_host_kernel_gs_base, vv->msr_guest_kernel_gs_base);
+}
+
+static void kdbx_vmx_dump_sel(char *name, uint32_t selector)
+{
+    uint32_t sel, attr, limit;
+    uint64_t base;
+
+    sel = vmcs_read32(selector);
+    attr = vmcs_read32(selector + (GUEST_ES_AR_BYTES - GUEST_ES_SELECTOR));
+    limit = vmcs_read32(selector + (GUEST_ES_LIMIT - GUEST_ES_SELECTOR));
+    base = vmcs_read64(selector + (GUEST_ES_BASE - GUEST_ES_SELECTOR));
+    kdbxp("%s: sel=0x%04x, attr=0x%05x, limit=0x%08x, base=0x%016lx\n",
+          name, sel, attr, limit, base);
+}
+
+static void kdbx_vmx_dump_sel2(char *name, uint32_t lim)
+{
+    uint32_t limit;
+    uint64_t base;
+
+    limit = vmcs_read32(lim);
+    base = vmcs_read64(lim + (GUEST_GDTR_BASE - GUEST_GDTR_LIMIT));
+    kdbxp("%s:                           limit=0x%08x, base=0x%016lx\n",
+          name, limit, base);
+}
+
+static void noinline kdbx_print_vmcs(void)
+{
+    kdbxp("*** Guest State ***\n");
+    kdbxp("CR0: actual=0x%016llx shadow=0x%016llx ghmask=%016llx\n",
+         (unsigned long long)vmcs_readl(GUEST_CR0),
+         (unsigned long long)vmcs_readl(CR0_READ_SHADOW), 
+         (unsigned long long)vmcs_readl(CR0_GUEST_HOST_MASK));
+    kdbxp("CR4: actual=0x%016llx shadow=0x%016llx ghmask=%016llx\n",
+         (unsigned long long)vmcs_readl(GUEST_CR4),
+         (unsigned long long)vmcs_readl(CR4_READ_SHADOW), 
+         (unsigned long long)vmcs_readl(CR4_GUEST_HOST_MASK));
+    kdbxp("CR3: actual=0x%016llx, target_count=%d\n",
+         (unsigned long long)vmcs_readl(GUEST_CR3),
+         vmcs_read32(CR3_TARGET_COUNT));
+    kdbxp("     target0=%016llx, target1=%016llx\n",
+         (unsigned long long)vmcs_readl(CR3_TARGET_VALUE0),
+         (unsigned long long)vmcs_readl(CR3_TARGET_VALUE1));
+    kdbxp("     target2=%016llx, target3=%016llx\n",
+         (unsigned long long)vmcs_readl(CR3_TARGET_VALUE2),
+         (unsigned long long)vmcs_readl(CR3_TARGET_VALUE3));
+    kdbxp("RSP = 0x%016llx RIP = 0x%016llx\n", 
+         (unsigned long long)vmcs_readl(GUEST_RSP),
+         (unsigned long long)vmcs_readl(GUEST_RIP));
+    kdbxp("RFLAGS=0x%016llx DR7 = 0x%016llx\n", 
+         (unsigned long long)vmcs_readl(GUEST_RFLAGS),
+         (unsigned long long)vmcs_readl(GUEST_DR7));
+    kdbxp("Sysenter RSP=%016llx CS:RIP=%04x:%016llx\n",
+         (unsigned long long)vmcs_readl(GUEST_SYSENTER_ESP),
+         vmcs_read32(GUEST_SYSENTER_CS),
+         (unsigned long long)vmcs_readl(GUEST_SYSENTER_EIP));
+    kdbx_vmx_dump_sel("CS", GUEST_CS_SELECTOR);
+    kdbx_vmx_dump_sel("DS", GUEST_DS_SELECTOR);
+    kdbx_vmx_dump_sel("SS", GUEST_SS_SELECTOR);
+    kdbx_vmx_dump_sel("ES", GUEST_ES_SELECTOR);
+    kdbx_vmx_dump_sel("FS", GUEST_FS_SELECTOR);
+    kdbx_vmx_dump_sel("GS", GUEST_GS_SELECTOR);
+    kdbx_vmx_dump_sel2("GDTR", GUEST_GDTR_LIMIT);
+    kdbx_vmx_dump_sel("LDTR", GUEST_LDTR_SELECTOR);
+    kdbx_vmx_dump_sel2("IDTR", GUEST_IDTR_LIMIT);
+    kdbx_vmx_dump_sel("TR", GUEST_TR_SELECTOR);
+    kdbxp("GUEST_PHYSICAL_ADDRESS: %lx\n", vmcs_read64(GUEST_PHYSICAL_ADDRESS));
+    kdbxp("Guest EFER = %016llx %016llx\n", vmcs_read64(GUEST_IA32_EFER),
+          vmcs_read64(GUEST_IA32_PAT));
+    kdbxp("TSC Offset = %016llx\n", vmcs_read64(TSC_OFFSET));
+    kdbxp("DebugCtl=%016llx DebugExceptions=%016llx\n", 
+           vmcs_read64(GUEST_IA32_DEBUGCTL),
+           (unsigned long long)vmcs_readl(GUEST_PENDING_DBG_EXCEPTIONS));
+    kdbxp("Interruptibility=%04x ActivityState=%04x\n",
+           vmcs_read32(GUEST_INTERRUPTIBILITY_INFO),
+           vmcs_read32(GUEST_ACTIVITY_STATE));
+
+    kdbxp("MSRs: entry_load:$%d exit_load:$%d exit_store:$%d\n",
+         vmcs_read32(VM_ENTRY_MSR_LOAD_COUNT), 
+         vmcs_read32(VM_EXIT_MSR_LOAD_COUNT),
+         vmcs_read32(VM_EXIT_MSR_STORE_COUNT));
+
+    kdbxp("\n*** Host State ***\n");
+    kdbxp("RSP = 0x%016llx  RIP = 0x%016llx\n", 
+           (unsigned long long)vmcs_readl(HOST_RSP),
+           (unsigned long long)vmcs_readl(HOST_RIP));
+    kdbxp("CS=%04x DS=%04x ES=%04x FS=%04x GS=%04x SS=%04x TR=%04x\n",
+           vmcs_read16(HOST_CS_SELECTOR),
+           vmcs_read16(HOST_DS_SELECTOR),
+           vmcs_read16(HOST_ES_SELECTOR),
+           vmcs_read16(HOST_FS_SELECTOR),
+           vmcs_read16(HOST_GS_SELECTOR),
+           vmcs_read16(HOST_SS_SELECTOR),
+           vmcs_read16(HOST_TR_SELECTOR));
+    kdbxp("FSBase=%016llx GSBase=%016llx TRBase=%016llx\n",
+           (unsigned long long)vmcs_readl(HOST_FS_BASE),
+           (unsigned long long)vmcs_readl(HOST_GS_BASE),
+           (unsigned long long)vmcs_readl(HOST_TR_BASE));
+    kdbxp("GDTBase=%016llx IDTBase=%016llx\n",
+           (unsigned long long)vmcs_readl(HOST_GDTR_BASE),
+           (unsigned long long)vmcs_readl(HOST_IDTR_BASE));
+    kdbxp("CR0=%016llx CR3=%016llx CR4=%016llx\n",
+           (unsigned long long)vmcs_readl(HOST_CR0),
+           (unsigned long long)vmcs_readl(HOST_CR3),
+           (unsigned long long)vmcs_readl(HOST_CR4));
+    kdbxp("Sysenter RSP=%016llx CS:RIP=%04x:%016llx\n",
+           (unsigned long long)vmcs_readl(HOST_IA32_SYSENTER_ESP),
+           vmcs_read32(HOST_IA32_SYSENTER_CS),
+           (unsigned long long)vmcs_readl(HOST_IA32_SYSENTER_EIP));
+    kdbxp("Host PAT = 0x%08x%08x\n",
+           vmcs_read32(HOST_IA32_PAT_HIGH), vmcs_read32(HOST_IA32_PAT));
+
+    kdbxp("\n*** Control State ***\n");
+    kdbxp("PinBased=%08x CPUBased=%08x SecondaryExec=%08x\n",
+           vmcs_read32(PIN_BASED_VM_EXEC_CONTROL),
+           vmcs_read32(CPU_BASED_VM_EXEC_CONTROL),
+           vmcs_read32(SECONDARY_VM_EXEC_CONTROL));
+    kdbxp("EntryControls=%08x ExitControls=%08x\n",
+           vmcs_read32(VM_ENTRY_CONTROLS), vmcs_read32(VM_EXIT_CONTROLS));
+    kdbxp("ExceptionBitmap=%08x\n", vmcs_read32(EXCEPTION_BITMAP));
+    kdbxp("PAGE_FAULT_ERROR_CODE  MASK:0x%lx  MATCH:0x%lx\n", 
+         vmcs_read32(PAGE_FAULT_ERROR_CODE_MASK),
+         vmcs_read32(PAGE_FAULT_ERROR_CODE_MATCH));
+    kdbxp("VMEntry: intr_info=%08x errcode=%08x ilen=%08x\n",
+           vmcs_read32(VM_ENTRY_INTR_INFO_FIELD),
+           vmcs_read32(VM_ENTRY_EXCEPTION_ERROR_CODE),
+           vmcs_read32(VM_ENTRY_INSTRUCTION_LEN));
+    kdbxp("VMExit: intr_info=%08x errcode=%08x ilen=%08x\n",
+           vmcs_read32(VM_EXIT_INTR_INFO),
+           vmcs_read32(VM_EXIT_INTR_ERROR_CODE),
+           vmcs_read32(VM_ENTRY_INSTRUCTION_LEN));
+    kdbxp("        reason=%08x qualification=%08x\n",
+           vmcs_read32(VM_EXIT_REASON), vmcs_readl(EXIT_QUALIFICATION));
+    kdbxp("IDTVectoring: info=%08x errcode=%08x\n",
+           vmcs_read32(IDT_VECTORING_INFO_FIELD),
+           vmcs_read32(IDT_VECTORING_ERROR_CODE));
+    kdbxp("TPR Threshold = 0x%02x\n", vmcs_read32(TPR_THRESHOLD));
+    kdbxp("EPT pointer = 0x%08x%08x\n", vmcs_read64(EPT_POINTER));
+    kdbxp("Virtual processor ID = 0x%04x\n", vmcs_read16(VIRTUAL_PROCESSOR_ID));
+    kdbxp("================================================================\n");
+}
+
+/* Flush VMCS on this cpu if it needs to: 
+ *   - Upon leaving kdb, the HVM cpu will resume in vmx_vmexit_handler() and 
+ *     do __vmreads. So, the VMCS pointer can't be left cleared.
+ *   - Doing __vmpclear will set the vmx state to 'clear', so to resume a
+ *     vmlaunch must be done and not vmresume. This means, we must clear 
+ *     arch_vmx->launched.
+ */
+void kdbx_curr_cpu_flush_vmcs(void)
+{
+    int ccpu = raw_smp_processor_id();
+    struct vmcs *vmcs = per_cpu(current_vmcs, ccpu);
+    struct kvm_vcpu *vcpu = vcpu_from_vmcs(vmcs);
+    struct vcpu_vmx *vmx = to_vmx(vcpu);
+
+    if (vmcs == NULL)
+        return;
+
+    if (vcpu == NULL) {
+        kdbxp("[%d]Bummer! Unable to find vcpu for vmcs:%p\n", ccpu, vmcs);
+        return;
+    }
+    if ( vmx->loaded_vmcs->launched == 0 )
+        return;
+
+    /* main kdbx cpu will load each vmcs and print it. so we just need to 
+     * make sure vmcs has what the cpu has. vmclear changes the launched 
+     * state to clear, after which a vmlaunch must be done, not vmresume */
+    vmcs_clear(vmcs);
+    vmx->loaded_vmcs->launched = 0;
+    vmcs_load(vmcs);
+}
+
+/* return : true if field succesfully found and loaded */
+static ulong kdbx_extract_vmcs_field(uint field)
+{
+    switch (field) {
+#if 0
+        case HOST_RSP :
+            // return vv->host_rsp;
+            return vmcs_read64(HOST_RSP);
+#endif
+
+        case GUEST_CR3:
+            return vmcs_readl(GUEST_CR3);
+
+        default:
+            kdbxp(">>>>>>>>>>>>> Illegal vmcs field:%d\n", field);
+    }
+    return 0xdeadbeefdeadbeef;
+}
+
+/* PreCondition: all cpus (including current cpu) have flushed VMCS */
+static ulong kdbx_vmcs_stuff(struct kvm_vcpu *in_vp, int field)
+{
+    extern void kdbx_cpu_flush_vmcs(int tgt_cpu);
+    int i;
+    struct list_head *lp;
+    ulong retval = 0xdeadbeefdeadbeef;
+    int ccpu = raw_smp_processor_id();
+    struct vmcs *vmcs = per_cpu(current_vmcs, ccpu);
+
+    if ( !enable_ept )
+        kdbxp(">>>> WARNING: ept is not enabled... \n");
+
+    if (vmcs) {
+        struct kvm_vcpu *vcpu = vcpu_from_vmcs(vmcs);
+        struct vcpu_vmx *vmx = to_vmx(vcpu);
+
+        vmcs_clear(vmcs);
+
+        if (vmx == NULL)
+            kdbxp("Unable to find vmx for vmcs:%\n", vmcs);
+        else
+            vmx->loaded_vmcs->launched = 0;
+    }
+
+    /* vm_list part of struct kvm in kvm_host.h */
+    list_for_each(lp, &vm_list) {
+        struct kvm *kp = list_entry(lp, struct kvm, vm_list);  /* container of*/
+
+        for (i = 0; i < KVM_MAX_VCPUS; i++) {
+            struct kvm_vcpu *vp = kp->vcpus[i];
+            struct vcpu_vmx *vx = to_vmx(vp); 
+
+            if ( in_vp && in_vp != vp )
+                continue;
+
+            if ( !vp || !vx->loaded_vmcs || !vx->loaded_vmcs->vmcs )
+                continue;
+
+            if ( vx->loaded_vmcs->launched ) {
+                if ( vx->loaded_vmcs->cpu == ccpu )
+                    kdbx_curr_cpu_flush_vmcs();
+                else
+                    kdbx_cpu_flush_vmcs(vx->loaded_vmcs->cpu);
+            }
+
+            vmcs_load(vx->loaded_vmcs->vmcs);
+
+            if ( field == ~0 ) {
+                kdbxp("vcpu:[id:%d]%p  vmcs:%p kvm:%p\n", vp->vcpu_id, vp,
+                      vx->loaded_vmcs->vmcs, kp);
+                kdbx_print_vmcs();
+                vmcs_clear(vx->loaded_vmcs->vmcs);
+            } else  {
+                retval = kdbx_extract_vmcs_field(field);
+                vmcs_clear(vx->loaded_vmcs->vmcs);
+                goto out;
+            }
+
+        }
+    }
+out:
+    if (vmcs)
+        vmcs_load(vmcs);
+
+    return retval;
+}
+
+void kdbx_dump_vmcs(struct kvm_vcpu *vp)
+{
+    kdbx_vmcs_stuff(vp, ~0);
+}
+
+ulong kdbx_get_vmcs_field(struct kvm_vcpu *vp, uint field)
+{
+    return kdbx_vmcs_stuff(vp, field);
+}
+
+#endif  /* CONFIG_KDBX */
diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
index 304d31d8cbbc..23a89c3048a9 100644
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@ -1525,6 +1525,12 @@ do_page_fault(struct pt_regs *regs, unsigned long error_code, unsigned long addr
 {
 	enum ctx_state prev_state;
 
+#ifdef CONFIG_KDBX
+        if (kdbx_session_begun) {
+            if ( kdbx_excp_fixup(regs, X86_TRAP_PF) == 0 )
+                return 0;
+        }
+#endif
 	prev_state = exception_enter();
 	trace_page_fault_entries(regs, error_code, address);
 	__do_page_fault(regs, error_code, address);
diff --git a/drivers/block/virtio_blk.c b/drivers/block/virtio_blk.c
index 7ffd719d89de..db85524e2fe1 100644
--- a/drivers/block/virtio_blk.c
+++ b/drivers/block/virtio_blk.c
@@ -1060,3 +1060,30 @@ module_exit(fini);
 MODULE_DEVICE_TABLE(virtio, id_table);
 MODULE_DESCRIPTION("Virtio block driver");
 MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_KDBX
+void kdbx_disp_virtio_blk(struct virtio_blk *vb)
+{
+        struct gendisk *gd = vb->disk;
+        struct blk_mq_tag_set *mqt = &vb->tag_set;
+
+        kdbxp("   virtio_blk{} is:\n");
+        if (gd)
+            kdbxp("    gendisk(%p): nm: %s  maj: %d  fmin: %d\n", 
+                  gd, gd->disk_name, gd->major, gd->first_minor);
+        if ( mqt ) {
+            kdbxp("    blk_mq_tag_set: %p\n", mqt);
+            kdbxp("      mq_ops funcs: queue: %s  softirq_done: %s\n",
+                  kdbx_hostsym(mqt->ops->queue_rq), 
+                  kdbx_hostsym(mqt->ops->complete));
+            kdbxp("        poll_fn: %s  map_queues_fn: %s\n",
+                  kdbx_hostsym(mqt->ops->poll), 
+                  kdbx_hostsym(mqt->ops->map_queues));
+            kdbxp("      nr_hw_queues: %d  depth: %d\n", 
+                  mqt->nr_hw_queues, mqt->queue_depth);
+        }
+        kdbxp("    sg_elems: %d  num_vqs: %d  blk_vq: %p\n",
+                  vb->sg_elems, vb->num_vqs, vb->vqs);
+        kdbxp("    config_work-func: %s\n", kdbx_hostsym(vb->config_work.func));
+}
+#endif
diff --git a/drivers/net/hyperv/netvsc.c b/drivers/net/hyperv/netvsc.c
index eab83e71567a..a15a1e80afae 100644
--- a/drivers/net/hyperv/netvsc.c
+++ b/drivers/net/hyperv/netvsc.c
@@ -1398,6 +1398,12 @@ struct netvsc_device *netvsc_device_add(struct hv_device *device,
 	netif_napi_add(ndev, &net_device->chan_table[0].napi,
 		       netvsc_poll, NAPI_POLL_WEIGHT);
 
+#ifdef __KDBX_SUPPORT_FOR_HYPERV
+{
+        extern void kdbx_add_netvsc(struct netvsc_device *net_device);
+        kdbx_add_netvsc(net_device);
+}
+#endif
 	/* Open the channel */
 	ret = vmbus_open(device->channel, netvsc_ring_bytes,
 			 netvsc_ring_bytes,  NULL, 0,
diff --git a/drivers/nvme/host/core.c b/drivers/nvme/host/core.c
index a6b7b242d516..687f534fd494 100644
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@ -86,7 +86,11 @@ static DEFINE_MUTEX(nvme_subsystems_lock);
 
 static DEFINE_IDA(nvme_instance_ida);
 static dev_t nvme_chr_devt;
+#ifdef CONFIG_KDBX
+struct class *nvme_class;
+#else
 static struct class *nvme_class;
+#endif
 static struct class *nvme_subsys_class;
 
 static int nvme_revalidate_disk(struct gendisk *disk);
diff --git a/drivers/nvme/host/pci.c b/drivers/nvme/host/pci.c
index 14d513087a14..8e23daf671fc 100644
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@ -3153,3 +3153,42 @@ MODULE_LICENSE("GPL");
 MODULE_VERSION("1.0");
 module_init(nvme_init);
 module_exit(nvme_exit);
+
+#ifdef CONFIG_KDBX
+extern void kdbx_prnt_tagset(struct blk_mq_tag_set *);
+extern struct class *nvme_class;
+
+void kdbx_dump_nvmes(void)
+{
+    struct class_dev_iter iter;
+    struct device *dev;          /* include/linux/device.h */
+    struct nvme_dev *ndev;
+    struct nvme_ctrl *nctrl;
+    struct nvme_ns *ns;
+
+    class_dev_iter_init(&iter, nvme_class, NULL, NULL);
+    while ((dev = class_dev_iter_next(&iter))) {
+        nctrl = container_of(dev, struct nvme_ctrl, ctrl_device);
+        ndev = container_of(nctrl, struct nvme_dev, ctrl);
+
+        kdbxp("nvme_dev:%p  namespaces:\n", ndev);
+        list_for_each_entry(ns, &nctrl->namespaces, list) {
+                struct gendisk *gd = ns->disk;
+                kdbxp("   gendisk:%p reqq: %p %s\n",gd,ns->queue,gd->disk_name);
+        }
+        if (ndev->queues)
+            kdbxp(" nvme_queue: q_depth:%u  cq_vector:%d  tagspp:%p\n",
+                  ndev->queues->q_depth, ndev->queues->cq_vector,
+                  ndev->queues->tags);
+        else
+            kdbxp("\n");
+
+        kdbxp(" online_queues:%u  q_depth:%u  max_qid:%u (all dec)\n", 
+              ndev->online_queues, ndev->q_depth, ndev->max_qid);
+        kdbxp(" subsystem:%1d  host_mem_size:%ul  nr_host_mem_descs:%u\n",
+              ndev->subsystem, ndev->host_mem_size, ndev->nr_host_mem_descs);
+        kdbx_prnt_tagset(&ndev->tagset);
+    }
+    class_dev_iter_exit(&iter);
+}
+#endif
diff --git a/drivers/scsi/virtio_scsi.c b/drivers/scsi/virtio_scsi.c
index bfec84aacd90..b767a097750a 100644
--- a/drivers/scsi/virtio_scsi.c
+++ b/drivers/scsi/virtio_scsi.c
@@ -1026,3 +1026,18 @@ module_exit(fini);
 MODULE_DEVICE_TABLE(virtio, id_table);
 MODULE_DESCRIPTION("Virtio SCSI HBA driver");
 MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_KDBX
+void kdbx_disp_virtio_scsi(struct virtio_scsi *vs)
+{
+    extern void kdbx_disp_virtio_device(struct virtio_device *, int);
+    int i;
+
+    kdbxp("    num_queues(req_vqs[]): %d\n", vs->num_queues);
+    kdbxp("    virtqueues:");
+    for (i=0; i < vs->num_queues; i++) {
+        kdbxp("        i:%d  req virtqueue:%p\n", i, vs->req_vqs[i].vq);
+    }
+    kdbx_disp_virtio_device(vs->vdev, 1);
+}
+#endif
diff --git a/drivers/tty/serial/8250/8250_port.c b/drivers/tty/serial/8250/8250_port.c
index 8407166610ce..4687c539f781 100644
--- a/drivers/tty/serial/8250/8250_port.c
+++ b/drivers/tty/serial/8250/8250_port.c
@@ -44,6 +44,10 @@
 #define UART_NPCM_TOR          7
 #define UART_NPCM_TOIE         BIT(7)  /* Timeout Interrupt Enable */
 
+#ifdef CONFIG_KDBX
+#include <asm/irq_regs.h>
+#endif
+
 /*
  * Debugging.
  */
@@ -1691,6 +1695,14 @@ void serial8250_read_char(struct uart_8250_port *up, unsigned char lsr)
 		else if (lsr & UART_LSR_FE)
 			flag = TTY_FRAME;
 	}
+#ifndef CONFIG_KDBX_FOR_XEN_DOM0
+#ifdef CONFIG_KDBX
+        if ( ch == 0x1c ) {
+                if ( kdbx_keyboard(get_irq_regs()) )
+                        return;
+        }
+#endif
+#endif
 	if (uart_prepare_sysrq_char(port, ch))
 		return;
 
diff --git a/drivers/tty/sysrq.c b/drivers/tty/sysrq.c
index 573b2055173c..a081b38c547a 100644
--- a/drivers/tty/sysrq.c
+++ b/drivers/tty/sysrq.c
@@ -132,6 +132,15 @@ static struct sysrq_key_op sysrq_unraw_op = {
 #define sysrq_unraw_op (*(struct sysrq_key_op *)NULL)
 #endif /* CONFIG_VT */
 
+#ifdef CONFIG_KDBX
+extern void kdbx_handle_sysrq_c(int);
+static struct sysrq_key_op sysrq_crash_op = {
+        .handler        = kdbx_handle_sysrq_c,
+        .help_msg       = "kdbx: run a cmd in kernel)",
+        .action_msg     = "",
+        .enable_mask    = SYSRQ_ENABLE_DUMP,
+};
+#else
 static void sysrq_handle_crash(int key)
 {
 	/* release the RCU read lock before crashing */
@@ -145,6 +154,7 @@ static struct sysrq_key_op sysrq_crash_op = {
 	.action_msg	= "Trigger a crash",
 	.enable_mask	= SYSRQ_ENABLE_DUMP,
 };
+#endif
 
 static void sysrq_handle_reboot(int key)
 {
diff --git a/drivers/virtio/virtio.c b/drivers/virtio/virtio.c
index a977e32a88f2..e98128e818ef 100644
--- a/drivers/virtio/virtio.c
+++ b/drivers/virtio/virtio.c
@@ -445,3 +445,10 @@ core_initcall(virtio_init);
 module_exit(virtio_exit);
 
 MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_KDBX
+struct bus_type *kdbx_ret_virtio_bus_addr(void)
+{
+    return (&virtio_bus);
+}
+#endif
diff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c
index 867c7ebd3f10..33700b3df933 100644
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@ -2327,3 +2327,36 @@ const struct vring *virtqueue_get_vring(struct virtqueue *vq)
 EXPORT_SYMBOL_GPL(virtqueue_get_vring);
 
 MODULE_LICENSE("GPL");
+
+#ifdef CONFIG_KDBX
+#include <linux/version.h>
+void kdbx_disp_virtq(struct virtqueue *vq)
+{
+    struct vring_virtqueue *vrq = to_vvq(vq);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5,4,0)
+    struct vring *vring = &vrq->vring;
+#endif
+
+    kdbxp("(displaying struct vring_virtqueue):\n");
+    kdbxp("virtqueue: %p  name:%s\n", vq, vq->name);
+    kdbxp("  callback: %s  virtio_device: %p\n",
+          kdbx_hostsym(vq->callback), vq->vdev);
+    kdbxp("  index: %d(0x%x)  num_free: %d(0x%x)\n", vq->index, vq->index,
+          vq->num_free, vq->num_free); 
+    kdbxp("  priv(notify mmio write va. ept fault on pa): %p\n", vq->priv);
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5,4,0)
+    kdbxp("vring: %p  num:%d/0x%x\n", vring, vring->num, vring->num);
+    kdbxp("  desc: %p  avail: %p  used: %p\n", vring->desc, vring->avail,
+          vring->used);
+    kdbxp("weak:%d  broken:%d indirect:%d event:%d\n", vrq->weak_barriers,
+          vrq->broken, vrq->indirect, vrq->event);
+    kdbxp("num_added: %x  last_used_idx: %x\n", vrq->num_added,
+          vrq->last_used_idx);
+    kdbxp("notify: %s  byte-q-size:%ld/0x%lx\n", kdbx_hostsym(vrq->notify),
+          vrq->queue_size_in_bytes, vrq->queue_size_in_bytes);
+    kdbxp("queue_dma_addr: %lx  desc_state: %p\n", vrq->queue_dma_addr,
+          vrq->desc_state);
+#endif
+#endif
+}
diff --git a/include/linux/printk.h b/include/linux/printk.h
index c09d67edda3a..bb3f99dc7195 100644
--- a/include/linux/printk.h
+++ b/include/linux/printk.h
@@ -8,6 +8,11 @@
 #include <linux/linkage.h>
 #include <linux/cache.h>
 
+#ifdef CONFIG_KDBX
+/* everybody includes printk.h, so put it here */
+#include "../../kdbx/include/kdbx_linux.h"
+#endif
+
 extern const char linux_banner[];
 extern const char linux_proc_banner[];
 
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3af98e8b8412..7cfa48de066f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1756,6 +1756,12 @@ static inline int test_tsk_thread_flag(struct task_struct *tsk, int flag)
 
 static inline void set_tsk_need_resched(struct task_struct *tsk)
 {
+#ifdef CONFIG_KDBX
+#if 0 /* not sure needed for 4.14 with all the preemptive changes */
+    if ( kdbx_session_begun )
+        return;
+#endif
+#endif
 	set_tsk_thread_flag(tsk,TIF_NEED_RESCHED);
 }
 
diff --git a/kernel/panic.c b/kernel/panic.c
index 45d78b1eace5..1f1d2f6fa7b0 100644
--- a/kernel/panic.c
+++ b/kernel/panic.c
@@ -174,6 +174,11 @@ void panic(const char *fmt, ...)
 	int old_cpu, this_cpu;
 	bool _crash_kexec_post_notifiers = crash_kexec_post_notifiers;
 
+#ifdef CONFIG_KDBX
+    kdbxp("Kernel Panic: %s\n", buf);
+    kdbx_trap_immed(KDBX_TRAP_FATAL);
+#endif
+
 	/*
 	 * Disable local interrupts. This will prevent panic_smp_self_stop
 	 * from deadlocking the first cpu that invokes the panic, since
diff --git a/kernel/time/timer.c b/kernel/time/timer.c
index 4820823515e9..cdda9a42c9ea 100644
--- a/kernel/time/timer.c
+++ b/kernel/time/timer.c
@@ -2089,3 +2089,55 @@ void __sched usleep_range(unsigned long min, unsigned long max)
 	}
 }
 EXPORT_SYMBOL(usleep_range);
+
+#ifdef CONFIG_KDBX
+void kdbx_dump_timer_queues(void)
+{
+    extern void kdbx_prnt_addr2sym(pid_t, ulong, char *);
+
+    int j, i, cpu;
+    char *skipfn = "br_multicast_port_group_expired";
+    ulong skipaddr = kallsyms_lookup_name(skipfn);
+
+    if ( skipaddr == 0 ) {
+        kdbxp("trq: note symbol: %s not found\n", skipfn);
+    } else {
+        kdbxp("trq: note skipping all timers for %s\n", skipfn);
+    }
+
+    kdbxp("jiffies: %llx tsc:%llx\n", jiffies_64, rdtsc());
+
+    for_each_possible_cpu(cpu) {
+        for (i = 0; i < NR_BASES; i++) {
+            struct timer_list *tlp;
+            struct timer_base *tbp = per_cpu_ptr(&timer_bases[BASE_STD], cpu);
+
+            kdbxp ("base:%p cpu: %d base: %d next_xpiry:%lx running:%lx\n",
+                   tbp, cpu, i, tbp->next_expiry, (ulong)tbp->running_timer);
+
+            for (j=0; j < WHEEL_SIZE; j++) {
+                struct hlist_node *p = tbp->vectors[j].first;
+
+                tlp = container_of(p, struct timer_list, entry);
+                while ( tlp ) {
+                    struct hlist_node *next = tlp->entry.next;
+
+                    if ( (ulong)tlp->function != skipaddr ) {
+                        kdbxp("  expires: %lx fn: ", tlp->expires);
+                        kdbx_prnt_addr2sym(0, (ulong)tlp->function, "\n");
+                    }
+
+                    if (next == NULL)
+                        break;
+
+                    tlp = container_of(next, struct timer_list, entry);
+                    if (tlp == tbp->running_timer)
+                        break;
+                }
+            }
+        }
+    }
+    kdbxp("\n");
+}
+#endif
+
diff --git a/kernel/watchdog.c b/kernel/watchdog.c
index f41334ef0971..ac3e957e9e1e 100644
--- a/kernel/watchdog.c
+++ b/kernel/watchdog.c
@@ -38,7 +38,12 @@ static DEFINE_MUTEX(watchdog_mutex);
 #endif
 
 unsigned long __read_mostly watchdog_enabled;
+#ifdef CONFIG_KDBX
+int __read_mostly watchdog_user_enabled = 0;
+#else
 int __read_mostly watchdog_user_enabled = 1;
+#endif
+
 int __read_mostly nmi_watchdog_user_enabled = NMI_WATCHDOG_DEFAULT;
 int __read_mostly soft_watchdog_user_enabled = 1;
 int __read_mostly watchdog_thresh = 10;
diff --git a/net/sched/sch_generic.c b/net/sched/sch_generic.c
index 7c3c5fdb82a9..bcabcf934eca 100644
--- a/net/sched/sch_generic.c
+++ b/net/sched/sch_generic.c
@@ -419,6 +419,9 @@ static void dev_watchdog(struct timer_list *t)
 {
 	struct net_device *dev = from_timer(dev, t, watchdog_timer);
 
+#ifdef CONFIG_KDBX
+        return;
+#endif
 	netif_tx_lock(dev);
 	if (!qdisc_tx_is_noop(dev)) {
 		if (netif_device_present(dev) &&
diff --git a/virt/kvm/eventfd.c b/virt/kvm/eventfd.c
index 27ae3219ae99..5bdcc2fd0d01 100644
--- a/virt/kvm/eventfd.c
+++ b/virt/kvm/eventfd.c
@@ -960,3 +960,21 @@ kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)
 
 	return kvm_assign_ioeventfd(kvm, args);
 }
+
+#ifdef CONFIG_KDBX
+void kvm_disp_ioeventfds(struct kvm *kp)
+{
+    struct _ioeventfd *p, *tmp;
+    char buf[KSYM_NAME_LEN+16];
+
+    kdbxp("\nioeventfds:\n");
+    kdbxp("    addr len dev.{read/write/destructor} bus_idx, wildcard\n");
+    list_for_each_entry_safe(p, tmp, &kp->ioeventfds, list) {
+        kdbxp("    %lx %d {%s/%s/%s} %d %d\n", p->addr, p->length,
+              kdbx_addr2sym(0, (ulong)p->dev.ops->read, buf, 0),
+              kdbx_addr2sym(0, (ulong)p->dev.ops->write, buf, 0),
+              kdbx_addr2sym(0, (ulong)p->dev.ops->destructor, buf, 0),
+              p->bus_idx, p->wildcard);
+    }
+}
+#endif
